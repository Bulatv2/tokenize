# tokenize russian
tokenize to senteces, words, punctuation, use stop-words, lemmatize, stemming

requirements: module vector from my repository
